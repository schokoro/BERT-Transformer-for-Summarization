{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from preprocess import LCSTSProcessor, DataLoader, DataProcessor\n",
    "from model import BertAbsSum\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from preprocess import convert_examples_to_features\n",
    "from tqdm import tqdm, trange\n",
    "from transformer import Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/03/2019 23:57:21 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/cross/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "12/03/2019 23:57:21 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/cross/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpjmrfp25g\n",
      "12/03/2019 23:57:26 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(dratf_logits, refine_logits, ground):\n",
    "    ground = ground[:, 1:]\n",
    "    draft_loss = F.cross_entropy(dratf_logits, ground, ignore_index=Constants.PAD)\n",
    "    refine_loss = F.cross_entropy(refine_logits, ground, ignore_index=Constants.PAD)\n",
    "    return draft_loss + refine_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS(object):\n",
    "    data_dir = 'data/processed_data'\n",
    "#     bert_model = 'chinese_L-12_H-768_A-12'\n",
    "    bert_model = 'bert-base-uncased'\n",
    "    output_dir = 'output'\n",
    "    GPU_index = 0\n",
    "    learning_rate = 5e-5\n",
    "    num_train_epochs = 3\n",
    "    warmup_proportion = 0.1\n",
    "    max_src_len = 130\n",
    "    max_tgt_len = 30\n",
    "    train_batch_size = 32\n",
    "    decoder_config = None\n",
    "    print_every = 100\n",
    "\n",
    "args = ARGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2019 00:08:50 - INFO - __main__ -   Using device:cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda', args.GPU_index)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "logger.info(f'Using device:{device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import pandas as pd\n",
    "# df = pd.read_csv('../competitions/title-generation/input/train.csv')\n",
    "df[['title', 'abstract']].head(20000).to_csv('data/processed_data', sep='\\t', header=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!mkdir bert-base-uncased\n",
    "bert_config = {\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"type_vocab_size\": 2,\n",
    "  \"vocab_size\": 30522\n",
    "}\n",
    "with open('bert-base-uncased/bert_config.json', 'w') as fobj:\n",
    "    json.dump(bert_config, fobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2019 00:17:39 - INFO - __main__ -   Saving model to output/model_12-04-00:17:39.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(args.output_dir):\n",
    "    os.makedirs(args.output_dir)\n",
    "model_path = os.path.join(args.output_dir, time.strftime('model_%m-%d-%H:%M:%S', time.localtime()))\n",
    "os.mkdir(model_path)\n",
    "logger.info(f'Saving model to {model_path}.')\n",
    "\n",
    "if args.decoder_config is not None:\n",
    "    with open(args.decoder_config, 'r') as f:\n",
    "        decoder_config = json.load(f)\n",
    "else:\n",
    "    with open(os.path.join(args.bert_model, 'bert_config.json'), 'r') as f:\n",
    "        bert_config = json.load(f)\n",
    "        decoder_config = {}\n",
    "        decoder_config['len_max_seq'] = args.max_tgt_len\n",
    "        decoder_config['d_word_vec'] = bert_config['vocab_size']\n",
    "        decoder_config['n_layers'] = 8\n",
    "        decoder_config['num_head'] = 12\n",
    "        decoder_config['d_k'] = 64\n",
    "        decoder_config['d_v'] = 64\n",
    "        decoder_config['d_model'] = bert_config['hidden_size']\n",
    "        decoder_config['d_inner'] = decoder_config['d_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2019 15:08:47 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/cross/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "12/04/2019 15:08:47 - INFO - __main__ -   Loading train examples...\n",
      "12/04/2019 15:08:47 - INFO - __main__ -   Converting train examples to features...\n",
      "examples: 100%|██████████| 20000/20000 [01:40<00:00, 199.71it/s]\n",
      "12/04/2019 15:10:28 - INFO - __main__ -   *** Example ***\n",
      "12/04/2019 15:10:28 - INFO - __main__ -   guid: 0\n",
      "12/04/2019 15:10:28 - INFO - __main__ -   src text: we consider the problem of utility maximization for investors with power utility functions. building on the earlier work larsen et al. (2016), we prove that the value of the problem is a frechet-differentiable function of the drift of the price process, provided that this drift lies in a suitable banach space.   we then study optimal investment problems with non-markovian driving processes. in such models there is no hope to get a formula for the achievable maximal utility. applying results of the first part of the paper we provide first order expansions for certain problems involving fractional brownian motion either in the drift or in the volatility. we also point out how asymptotic results can be derived for models with strong mean reversion.\n",
      "12/04/2019 15:10:28 - INFO - __main__ -   src_ids: 101 2057 5136 1996 3291 1997 9710 20446 3989 2005 9387 2007 2373 9710 4972 1012 2311 2006 1996 3041 2147 20094 3802 2632 1012 1006 2355 1007 1010 2057 6011 2008 1996 3643 1997 1996 3291 2003 1037 10424 27635 2102 1011 2367 19210 3853 1997 1996 11852 1997 1996 3976 2832 1010 3024 2008 2023 11852 3658 1999 1037 7218 7221 6776 2686 1012 2057 2059 2817 15502 5211 3471 2007 2512 1011 28003 18073 4439 6194 1012 1999 2107 4275 2045 2003 2053 3246 2000 2131 1037 5675 2005 1996 9353 4048 13331 3468 29160 9710 1012 11243 3463 1997 1996 2034 2112 1997 1996 3259 2057 3073 2034 2344 4935 2015 2005 3056 3471 5994 12884 2389 2829 2937 4367 2593 1999 1996 11852 2030 102\n",
      "12/04/2019 15:10:28 - INFO - __main__ -   src_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/04/2019 15:10:28 - INFO - __main__ -   tgt text: on optimal investment with processes of long or negative memory\n",
      "12/04/2019 15:10:28 - INFO - __main__ -   tgt_ids: 101 2006 15502 5211 2007 6194 1997 2146 2030 4997 3638 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2019 15:10:28 - INFO - __main__ -   tgt_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/04/2019 15:10:28 - INFO - __main__ -   Building dataloader...\n"
     ]
    }
   ],
   "source": [
    "# data preprocess\n",
    "processor = LCSTSProcessor()\n",
    "# processor = DataProcessor()\n",
    "tokenizer = BertTokenizer.from_pretrained(args.bert_model)\n",
    "logger.info('Loading train examples...')\n",
    "train_examples = processor.get_examples('data/processed_data')\n",
    "num_train_optimization_steps = int(len(train_examples) / args.train_batch_size) * args.num_train_epochs\n",
    "logger.info('Converting train examples to features...')\n",
    "features = convert_examples_to_features(train_examples, args.max_src_len, args.max_tgt_len, tokenizer)\n",
    "example = train_examples[0]\n",
    "example_feature = features[0]\n",
    "logger.info(\"*** Example ***\")\n",
    "logger.info(\"guid: %s\" % (example.guid))\n",
    "logger.info(\"src text: %s\" % example.src)\n",
    "logger.info(\"src_ids: %s\" % \" \".join([str(x) for x in example_feature.src_ids]))\n",
    "logger.info(\"src_mask: %s\" % \" \".join([str(x) for x in example_feature.src_mask]))\n",
    "logger.info(\"tgt text: %s\" % example.tgt)\n",
    "logger.info(\"tgt_ids: %s\" % \" \".join([str(x) for x in example_feature.tgt_ids]))\n",
    "logger.info(\"tgt_mask: %s\" % \" \".join([str(x) for x in example_feature.tgt_mask]))\n",
    "logger.info('Building dataloader...')\n",
    "all_src_ids = torch.tensor([f.src_ids for f in features], dtype=torch.long)\n",
    "all_src_mask = torch.tensor([f.src_mask for f in features], dtype=torch.long)\n",
    "all_tgt_ids = torch.tensor([f.tgt_ids for f in features], dtype=torch.long)\n",
    "all_tgt_mask = torch.tensor([f.tgt_mask for f in features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_src_ids, all_src_mask, all_tgt_ids, all_tgt_mask)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/04/2019 15:11:10 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /home/cross/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "12/04/2019 15:11:10 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/cross/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpyuilteld\n",
      "12/04/2019 15:11:15 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'n_head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-b5057de92d65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertAbsSum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/BERT-Transformer-for-Summarization/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bert_model_path, decoder_config, device)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteacher_forcing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/BERT-Transformer-for-Summarization/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, decoder_config, embedding, device, dropout)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0md_word_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd_word_vec'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mn_head\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_head'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0md_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd_k'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0md_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'd_v'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'n_head'"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = BertAbsSum(args.bert_model, decoder_config, device)\n",
    "model.to(device)\n",
    "\n",
    "# optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=args.learning_rate,\n",
    "                     warmup=0.1,\n",
    "                     t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_config['n_head']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "model.train()\n",
    "global_step = 0\n",
    "for i in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        draft_logits, refine_logits = model(*batch)\n",
    "        loss = cal_loss(draft_logits, refine_logits, batch[2])\n",
    "        loss.backward()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += batch[0].size(0)\n",
    "        nb_tr_steps += 1\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "    if step % args.print_every == 0:\n",
    "        logger.info(f'Epoch {i}, step {step}, loss {loss.item()}.')\n",
    "    torch.save(model.state_dict(), os.join(model_path, 'BertAbsSum.bin'))\n",
    "    logger.info(f'Epoch {i} finished. Model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
